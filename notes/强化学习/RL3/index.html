<!DOCTYPE html>
<html lang="zh-CN">

<head>
	<meta name="generator" content="Hugo 0.62.0" />
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">




<meta itemprop="name" content="Finite Markov Decision Processes">
<meta itemprop="description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 3 感觉这一章就给了一些基础概念，看西瓜书也成啊 3.1 The Agent–E">

<meta itemprop="wordCount" content="1843">



<meta itemprop="keywords" content="" />
<meta property="og:title" content="Finite Markov Decision Processes" />
<meta property="og:description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 3 感觉这一章就给了一些基础概念，看西瓜书也成啊 3.1 The Agent–E" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl3/" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Finite Markov Decision Processes"/>
<meta name="twitter:description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 3 感觉这一章就给了一些基础概念，看西瓜书也成啊 3.1 The Agent–E"/>

<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
<title>Finite Markov Decision Processes</title>
<link rel="stylesheet" href="/css/bootstrap.materia.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<link rel="stylesheet" href="//at.alicdn.com/t/font_1604402_jsamxf8dml9.css">

<link rel="stylesheet" href="/sass/main.min.b85c780ce29d94800879b7f7d03bdbd1ed6e538ce8a520b9c374570f8c6ce49a.min.8e2aeef93a541530729538cb32ede52cfe40cf74307f450f1a6e5a4acd47ebb0.css" integity="sha256-jiru&#43;TpUFTBylTjLMu3lLP5Az3Qwf0UPGm5aSs1H67A=">
</head>

<body style="overflow-x: unset;">
	<div class="container-fluid">
		<div class="row d-print-block">
			<div class="col-12 col-md-3 col-lg-2 bd-sidebar d-print-none">
				<div class="d-flex mt-3 border-bottom">
        <span class="navbar-brand w-100" style="display: grid;">
            <small>
                <a href="/" class="text-black-50">
                    <i class="iconfont icon-back-arrow-"></i>
                </a>
                TristonK's
            </small>
            <a class="text-dark" href="/notes/">
                Documents
            </a>
        </span>
        <button class="btn btn-link text-dark d-md-none p-0 ml-3" type="button" data-toggle="collapse"
            data-target="#bd-docs-nav" aria-controls="bd-docs-nav" aria-expanded="true"
            aria-label="Toggle docs navigation">
            <i class="fad fa-bars"></i>
        </button>
    </div>
				<nav id="bd-docs-nav" class="collapse bd-links">
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E5%BE%AE%E7%94%B5%E5%AD%90%E4%B8%8E%E7%94%B5%E8%B7%AF/">
            
            
                
                    <span class="icontext">O</span>
                
            
            
            微电子与电路
        </a>
    </div>
    
    
    
    <div class="bd-toc-item active bg-light">
        <a class="bd-toc-link" href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
            <i class="iconfont icon-back-arrow-reverse"></i>
            强化学习
        </a>
        <ul class="nav bd-sidenav">
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/">Multi-armed Bandits</a>
            </li>
            
            
            
            <li class="active">
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl3/">
                    <i class="fad fa-chevron-right mr-1"></i>
                    Finite Markov Decision Processes
                </a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl4/">Dynamic Programming</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl5/">Monte Carlo Methods</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl6/">Temporal-Difference Learning</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl7/">n-step Bootstraping</a>
            </li>
            
            
        </ul>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%A4%BE%E4%BC%9A%E5%AD%A6%E6%A6%82%E8%AE%BA/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            社会学概论
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            组合数学
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%AE%BA/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            数据库概论
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            计算机网络
        </a>
    </div>
    
    
</nav>
			</div>
			<div class="col-12 col-md-9 col-lg-10 d-print-block">
				<div class="row d-print-block">
					<main class="col-12 col-md-10 col-lg-9 py-md-3 pl-md-5 bd-content d-print-block" role="main">
						<div id="title" class="my-4 border-bottom">
							<span>强化学习</span>
							<h2>Finite Markov Decision Processes</h2>
							<footer>
								<span>
									<i class="fad fa-calendar-alt mr-2"></i>
									0001-01-01 08:00 CST
								</span> <br />
								<span>
									<i class="fad fa-calendar-edit mr-2"></i>
									0001-01-01 08:00 CST
								</span> <br />
								<span>
									<i class="fad fa-copyright mr-2"></i>
									CC BY-NC 4.0
									<a class="float-right" href="#" onclick="window.print()">
											打印本页
									</a>
								</span>
							</footer>
						</div>
						<div id="content" class="hl-h2">
							<blockquote>
<p>对应章节：<a href="http://www.incompleteideas.net/book/the-book-2nd.html">《Reinforcement Learning: An Introduction》第二版</a>  Chap 3</p>
</blockquote>
<p>感觉这一章就给了一些基础概念，看西瓜书也成啊</p>
<h2 id="31-the-agentenvironment-interface">3.1 The Agent–Environment Interface<a href="#31-the-agentenvironment-interface" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>The learner and decision maker is called the <strong><code>agent</code></strong>. The thing it interacts with, comprising everything outside the agent, is called the <strong><code>environment</code></strong>.</p>
<img src="RL3/31.png">
<p>One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1}\in \textbf{R}$, and finds itself in a new state, $S_{t+1}$.</p>
<p>定义：$p(s&rsquo;, r|s, a)= Pr{S_t=s&rsquo;,R_t=r | S_{t−1}=s,A_{t−1}=a}$
The function p defines the **<code>dynamics</code>** of the **<code>MDP</code>**</p>
<p>可以看出在马尔可夫决策过程中，每一个的概率只依靠于前一次的状态于动作</p>
<p>类似的，可以定义以下函数：</p>
<img src="RL3/32.png" width=75% length=75%>
<p>The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.</p>
<p>然后举了几个简单的例子，有兴趣可以去看</p>
<h2 id="32-goals-and-rewards">3.2 Goals and Rewards<a href="#32-goals-and-rewards" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>在强化学习中，agent的目标是获取最大化的奖励和。这意味着不能单单只看立即得到的眼前的奖励，而要看长远过程下的奖励之和</p>
<p>奖励信号更应该告诉agent最终目标，而不是其实现过程，如同下棋时，应当是当你赢时给与奖励而不是局部最优时给予，如果那样设定的话可能导致agent追求局部最优而忽略全局</p>
<h2 id="33-returns-and-episodes">3.3 Returns and Episodes<a href="#33-returns-and-episodes" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>In general, we seek to maximize the <strong><code>expected return</code></strong>, where the return, denoted $G_t$, is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards:$G_t=R_{t+1} + R_{t+2} + R_{t+3} + · · · + R_T$ , where T is a final time step.</p>
<p>This approach makes sense in applications in which there is a natural notion of final time step, that is, when the agent–environment interaction breaks naturally into subsequences, which we call <strong><code>episodes</code></strong>(有时也用trials)</p>
<p>每一个episode都有一个终止状态，终止状态后将被重置为初始状态</p>
<p>Tasks with episodes of this kind are called <strong><code>episodic tasks</code></strong>（举例：下棋，走迷宫）. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted $S$, from the set of all states plus the terminal state, denoted $S^+$. The time of termination, $T$, is a random variable that normally varies from episode to episode</p>
<p>但也有些任务是一直不会终止的，我们称为<strong><code>continuing tasks</code></strong>, 这样的话我们的$G_t$将会是无限大，故而我们采取一种概念上复杂但是数学上简单的<strong><code>discount</code></strong>定义</p>
<p>$$G_t=R_{t+1} +\gamma R_{t+2} +\gamma^2 R_{t+3} + · · · =R_{t+1} + \gamma G_{t+1}$$</p>
<p>即discounting rate $0 \leq \gamma\leq 1$，决定了未来奖励的当前价值。</p>
<h2 id="34-unified-notation-for-episodic-and-continuing-tasks">3.4 Unified Notation for Episodic and Continuing Tasks<a href="#34-unified-notation-for-episodic-and-continuing-tasks" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>在之后的介绍中，我们两种任务都会探讨，所以我们希望寻求一种能够通用的表示两种任务的记法</p>
<p>我们可以考虑让episode tasks 在进入终止状态后进入一个特殊的 <strong><code>absorbing state</code></strong>，在这个状态下永远只转移到自身状态并且reward永为0，如下图深色方框：
<img src="RL3/33.png"></p>
<p>故而可定义$$Gt=\sum_{k=t+1}^T\gamma^{k−t−1}R_k$$，这个公式包含了$T = \infin$ or $\gamma = 1$ (but not both)的可能性.</p>
<h2 id="35-policies-and-value-functions">3.5 Policies and Value Functions<a href="#35-policies-and-value-functions" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p><strong><code>value functions</code></strong>—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p>
<p><strong><code>policy</code></strong> is a mapping from states to probabilities of selecting each possible action.</p>
<p>定义：</p>
<p>state-value function for policy : $v_\pi(s) = E_\pi(G_t | S_t = s) = E_\pi( \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t =s)$</p>
<p>action-value function for policy: $q_\pi(s, a) = E_\pi(G_t | S_t = s, A_t = a) = E_\pi( \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t =s, A_t = a)$</p>
<p>我们一般可以通过经验（大量随机实验样本的均值）来获取其估计值，（如Monte Carlo methods），但是当状态空间很大的时候，这是不现实的，那时可以将其当作参数方程，通过调参的方式来得到返回值，其精确度取决于近似方式</p>
<p>满足递归关系, 称为是Bellman equation for $v_\pi$, It expresses a relationship between the value of a state and the values of its successor states.
<img src="RL3/34.png"></p>
<p>如果已知参数（p(s′,r|s,a),π等), 则可以视为线性方程组，对于小规模的状态集，可以直接求解，其解就是值函数</p>
<p>对应的backup diagram：</p>
<img src="RL3/35.png">
<h2 id="36-optimal-policies-and-optimal-value-functions">3.6 Optimal Policies and Optimal Value Functions<a href="#36-optimal-policies-and-optimal-value-functions" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>optimal policy：$\pi_*$</p>
<p>optimal state-value function, denoted $v_*$ and defined as $v_*(s) = \max_\pi v_\pi(s)$</p>
<p>optimal action-value function, denoted $q_*$, and defined as $q_*(s,a) = max_\pi q_\pi(s,a)$</p>
<p>有关系： $q_*(s, a) = E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s,A_t=a]$ .</p>
<p>因为$v_*$也是policy的value-function，故而其也满足贝尔曼方程的条件，称为**<code>Bellman optimality equation</code>**.</p>
<p>可以写作：
<img src="RL3/36.png"></p>
<p>最后两行（3.18，3.19）是两种常见的不同的表达形式</p>
<p>对应的有：
<img src="RL3/37.png"></p>
<p>对应的backup diagram为：
<img src="RL3/38.png" width=75%></p>
<p>For finite MDPs, the Bellman optimality equation for $v_{*}$ has a unique solution。如果有n个状态，那么就得到一个n个变量的n个等式，如果知道必要的参数，可以通过解非线性方程组的方式对其进行求解</p>
<p>解出来后：
If you have the optimal value function, $v_{*}$, then the actions that appear best after a one-step search will be optimal actions. Another way of saying this is that any policy that is greedy with respect to the optimal evaluation function $v_{*}$ is an optimal policy.</p>
<p>With $q_{*}$, the agent does not even have to do a one-step-ahead search: for any state s, it can simply find any action that maximizes $q_{*}(s, a)$.</p>
<img src="RL3/39.png" width=75%>
<p>但是这种方式的缺点在于：我们需要精确的知道环境的动态性质且有足够的计算资源去完成所有运算，同时环境需满足马尔科夫性质</p>
<blockquote>
<p>Many different decision-making methods can be viewed as ways of approximately solving the Bellman optimality equation. For example, heuristic search methods can be viewed as expanding the right-hand side of (3.19) several times, up to some depth, forming a “tree” of possibilities, and then using a heuristic evaluation function to approximate $v_{*}$ at the “leaf” nodes. (Heuristic search methods such as $A^*$ are almost always based on the episodic case.) The methods of dynamic programming can be related even more closely to the Bellman optimality equation. Many reinforcement learning methods can be clearly understood as approximately solving the Bellman optimality equation, using actual experienced transitions in place of knowledge of the expected transitions. We consider a variety of such methods in the following chapters.</p>
</blockquote>

						</div>
					</main>
					<div class="d-none d-lg-block col-lg-3 bd-toc d-print-none">
						<div class="btn-group-vertical w-100 my-3">
    <a class="btn btn-outline-secondary text-dark w-100 p-2" href="https://github.com/shelah-kuang/shelah-kuang.github.io/issues" target="_blank">
        <i class="iconfont icon-LC_icon_list_line"></i><br />待更新列表
    </a>
    <a class="btn btn-outline-secondary text-dark w-100 p-2" href="mailto:kuangsl@foxmail.com"
        target="_blank">
        <i class="iconfont icon-discussion"></i><br />纠错与咨询
    </a>
</div>
						<nav id="TableOfContents">
  <ul>
    <li><a href="#31-the-agentenvironment-interface">3.1 The Agent–Environment Interface</a></li>
    <li><a href="#32-goals-and-rewards">3.2 Goals and Rewards</a></li>
    <li><a href="#33-returns-and-episodes">3.3 Returns and Episodes</a></li>
    <li><a href="#34-unified-notation-for-episodic-and-continuing-tasks">3.4 Unified Notation for Episodic and Continuing Tasks</a></li>
    <li><a href="#35-policies-and-value-functions">3.5 Policies and Value Functions</a></li>
    <li><a href="#36-optimal-policies-and-optimal-value-functions">3.6 Optimal Policies and Optimal Value Functions</a></li>
  </ul>
</nav>
					</div>
				</div>
			</div>
		</div>
	</div>

	<script type="text/javascript" src="/js/jquery-3.4.1.min.js"></script>
<script type="text/javascript" src="/js/bootstrap.bundle.min.js"></script>
<script type="text/javascript" src="/js/leon.js"></script>
<script type="text/javascript" src="/js/TweenMax.min.js "></script>
<script type="text/javascript" src="/js/search.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    
<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(
            document.body, {
                delimiters: [{
                        left: "$$",
                        right: "$$",
                        display: true
                    },
                    {
                        left: "\\[",
                        right: "\\]",
                        display: true
                    },
                    {
                        left: "$",
                        right: "$",
                        display: false
                    },
                    {
                        left: "\\(",
                        right: "\\)",
                        display: false
                    }
                ]
            }
        );
    });
</script>

<script type="text/javascript">
    $(document).on('click', 'a[href^="#"]', function (event) {
        event.preventDefault();

        $('html, body').animate({
            scrollTop: $($.attr(this, 'href')).offset().top
        }, 500);
    });
</script>


<script type="text/javascript">
    $(function () {
        $("[data-toggle='tooltip']").tooltip();
    });
</script>


</body>

</html>