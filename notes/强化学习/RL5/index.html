<!DOCTYPE html>
<html lang="zh-CN">

<head>
	<meta name="generator" content="Hugo 0.62.0" />
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">




<meta itemprop="name" content="Monte Carlo Methods">
<meta itemprop="description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 5 蒙特卡洛方法不需要环境的信息，只需要经验（experience">

<meta itemprop="wordCount" content="1783">



<meta itemprop="keywords" content="" />
<meta property="og:title" content="Monte Carlo Methods" />
<meta property="og:description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 5 蒙特卡洛方法不需要环境的信息，只需要经验（experience" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl5/" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Monte Carlo Methods"/>
<meta name="twitter:description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 5 蒙特卡洛方法不需要环境的信息，只需要经验（experience"/>

<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
<title>Monte Carlo Methods</title>
<link rel="stylesheet" href="/css/bootstrap.materia.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<link rel="stylesheet" href="//at.alicdn.com/t/font_1559566_z93lvlj24ja.css">

<link rel="stylesheet" href="/sass/main.min.28c01c661befd515dddc5c231b0895ce15c078e0b7f29c8c9a3ebe3847d4557c.min.f26af5aa60d3988015c49059001da55420563ad7781f34b06765e2a8c3dcec75.css" integity="sha256-8mr1qmDTmIAVxJBZAB2lVCBWOtd4HzSwZ2XiqMPc7HU=" media="screen">
</head>

<body style="overflow-x: unset;">
	<div class="container-fluid">
		<div class="row">
			<div class="col-12 col-md-3 col-lg-2 bd-sidebar d-print-none">
				<div class="d-flex mt-3 border-bottom">
        <span class="navbar-brand w-100" style="display: grid;">
            <small>
                <a href="/" class="text-black-50">
                    <i class="iconfont icon-back-arrow-"></i>
                </a>
                TristonK's
            </small>
            <a class="text-dark" href="/notes/">
                Documents
            </a>
        </span>
        <button class="btn btn-link text-dark d-md-none p-0 ml-3" type="button" data-toggle="collapse"
            data-target="#bd-docs-nav" aria-controls="bd-docs-nav" aria-expanded="true"
            aria-label="Toggle docs navigation">
            <i class="fad fa-bars"></i>
        </button>
    </div>
				<nav id="bd-docs-nav" class="collapse bd-links">
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E5%BE%AE%E7%94%B5%E5%AD%90%E4%B8%8E%E7%94%B5%E8%B7%AF/">
            
            
                
                    <span class="icontext">M</span>
                
            
            
            微电子与电路
        </a>
    </div>
    
    
    
    <div class="bd-toc-item active bg-light">
        <a class="bd-toc-link" href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
            <i class="iconfont icon-back-arrow-reverse"></i>
            强化学习
        </a>
        <ul class="nav bd-sidenav">
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/">Multi-armed Bandits</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl3/">Finite Markov Decision Processes</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl4/">Dynamic Programming</a>
            </li>
            
            
            
            <li class="active">
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl5/">
                    <i class="fad fa-chevron-right mr-1"></i>
                    Monte Carlo Methods
                </a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl6/">Temporal-Difference Learning</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl7/">n-step Bootstraping</a>
            </li>
            
            
        </ul>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%A4%BE%E4%BC%9A%E5%AD%A6%E6%A6%82%E8%AE%BA/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            社会学概论
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            组合数学
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%AE%BA/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            数据库概论
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">
            
                <i class="iconfont icon-construction rawicon"></i>
            
            计算机网络
        </a>
    </div>
    
    
</nav>
			</div>
			<div class="col-12 col-md-9 col-lg-10 d-print-block">
				<div class="row">
					<main class="col-12 col-md-10 col-lg-9 py-md-3 pl-md-5 bd-content" role="main">
						<div id="title" class="my-4 border-bottom">
							<span>强化学习</span>
							<h2>Monte Carlo Methods</h2>
							<footer>
								<span>
									<i class="fad fa-calendar-alt mr-2"></i>
									0001-01-01 08:00 CST
								</span> <br />
								<span>
									<i class="fad fa-calendar-edit mr-2"></i>
									0001-01-01 08:00 CST
								</span> <br />
								<span>
									<i class="fad fa-copyright mr-2"></i>
									CC BY-NC 4.0
								</span>
							</footer>
						</div>
						<div id="content" class="hl-h2">
							<blockquote>
<p>对应章节：<a href="http://www.incompleteideas.net/book/the-book-2nd.html">《Reinforcement Learning: An Introduction》第二版</a>  Chap 5</p>
</blockquote>
<p>蒙特卡洛方法不需要环境的信息，只需要经验（experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.）</p>
<p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks.</p>
<h2 id="51-monte-carlo-prediction">5.1 Monte Carlo Prediction<a href="#51-monte-carlo-prediction" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>有两种方法，一种是“<strong><code>first-visit</code></strong>“,把整个片段集所有第一次访问到状态<img src="https://www.zhihu.com/equation?tex=s" alt="[公式]">时的returns做平均化处理，来估计$v_\pi(s)$的值另一种是”**<code>every visit</code>**”，把整个片段集中所有访问到s状态时的returns取平均，来估计$v_\pi(s)$的值。两种方法都有应用，下给出first-visit，在9和12章介绍every-visit</p>
<img src="RL5/51.png">
<p>每一次返回平均值都是其本身的无偏估计，标准偏差在$\sqrt{\frac{1}{n}}$内</p>
<p>蒙特卡洛算法每一次运行都是独立的，也就是说他不是&quot;bootstrap&quot;的</p>
<h2 id="52-monte-carlo-estimation-of-action-values">5.2 Monte Carlo Estimation of Action Values<a href="#52-monte-carlo-estimation-of-action-values" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>如何估计$q_\pi(s,a)$, 同上一小节一样也是first-visit与every-visit两种方式</p>
<p>不过有一个问题在于如何保证所有的action都被考虑到，这就是之前第二章提到的如何保持exploit和explore的关系的问题，称为“<strong><code>maintaining exloration</code></strong>“问题，一种解决方式是对于每一个state–action pair，都给予其一定的概率作为一个episode的起点，这样当取样次数趋于无限的时候，每一个pair的取样次数也趋于无限了，这种方式的假设称为<strong><code>exploring starts</code></strong>.
但是这样有一个问题就是&quot;it cannot be relied upon in general, particularly when learning directly from actual interaction with an environment.&quot;, 此时一种常见的策略是调整policy，使得对于一个状态，所有的action都有一定的概率发生</p>
<h2 id="53-monte-carlo-control">5.3 Monte Carlo Control<a href="#53-monte-carlo-control" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>蒙特卡洛方法估计最优policies的方法和DP差不多：先迭代value function使之逼近当前policy的真实value function，然后基于更新后的value function进行policy improvement，直到最终policy基本不再变化</p>
<img src="RL5/52.png">
<p>由于我们在此处估计的是$q_\pi(s,a)$，就不需要额外的model来确定最优的action了，可以直接根据$\pi(s)=\arg \max_a q(s, a)$来确定</p>
<p>为了更具备实际意义，我们要考虑如何去掉infinte number of episodes 的假设 ，一种方式是设立一个极小值，当两次policy evaluation的差别小于这个值的时候，认为此次policy evaluation结束，这种方式在小规模情况下很好，但是数据规模较大时仍然会需要比较多次。另一种方式就是放弃完整的policy evaluation，类似于4.6中的value  iteration</p>
<p>以下算法仍然基于了exploring starts 假设</p>
<img src="RL5/53.png">
<h2 id="54-monte-carlo-control-without-exploring-starts">5.4 Monte Carlo Control without Exploring Starts<a href="#54-monte-carlo-control-without-exploring-starts" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p><strong><code>on-policy</code></strong>: 直接优化或评价目标策略</p>
<p>对于on-policy策略而言，对于所有的pair有$\pi(a|s) \ge \frac{\varepsilon}{|\mathcal A(s)|}$</p>
<p>采取第二章中提到的类似的方法，以$p= 1- \varepsilon + \frac{\varepsilon}{|\mathcal A(s)|}$的概率选取原先确定的action</p>
<img src="RL5/54.png">
<p><del>此处省略一大段，如果有兴趣看相关证明的可以自己看书</del></p>
<h2 id="55-off-policy-prediction-via-importance-sampling">5.5 Off-policy Prediction via Importance Sampling<a href="#55-off-policy-prediction-via-importance-sampling" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<p>在on-policy中，由于我们需要explore所有的action，这也就导致我们在一些时候会选择非最优的情况，故而我们有了off-policy的想法</p>
<p><strong><code>off-policy</code></strong>: 有两个策略，一个叫行为策略 $b$（behavior policy），另一个叫做目标策略$\pi$（target policy），从behavior policy生成的episodes中学习target policy的过程，叫做off-policy learning。</p>
<blockquote>
<p>关于off-policy与on-policy的应用比较</p>
<p>Throughout the rest of this book we consider both on-policy and off-policy methods. On-policy methods are generally simpler and are considered first. Off-policy methods require additional concepts and notation, and because the data is due to a di↵erent policy, off-policy methods are often of greater variance and are slower to converge. On the other hand, off-policy methods are more powerful and general.They include on-policy methods as the special case in which the target and behavior policies are the same. Off-policy methods also have a variety of additional uses in applications. For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert. Off-policy learning is also seen by some as key to learning multi-step predictive models of the world’s dynamics</p>
</blockquote>
<p><del>下面又是一大波我不想看的数学知识</del></p>
<p>TODO()<del>以后一定补</del></p>
<p>因此有两种方式，可以取：</p>
<p>一种是取平均（ordinary importance sampling）</p>
<p>$$V(s) = \frac{\sum_{t \in \mathcal T(s)} \rho_t G_t}{|\mathcal T(s)|}$$</p>
<p>另一种是加权平均（weighted average）</p>
<p>$$V(s) = \frac{\sum_{t \in \mathcal T(s)} \rho_t G_t}{\sum_{t \in \mathcal T(s)} \rho_t}$$</p>
<h2 id="56-incremental-implementation">5.6 Incremental Implementation<a href="#56-incremental-implementation" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<img src="RL5/55.png">
<h2 id="57-off-policy-monte-carlo-control">5.7 Off-policy Monte Carlo Control<a href="#57-off-policy-monte-carlo-control" class="anchor" aria-hidden="true"><i class="iconfont icon-link"></i></a></h2>
<img src="RL5/56.png">
						</div>
					</main>
					<div class="d-none d-lg-block col-lg-3 bd-toc d-print-none">
						<div class="btn-group-vertical w-100 my-3">
    <a class="btn btn-outline-secondary text-dark w-100 p-2" href="https://github.com/shelah-kuang/shelah-kuang.github.io/commits/master" target="_blank">
        <i class="iconfont icon-LC_icon_list_line"></i><br />待更新列表
    </a>
    <a class="btn btn-outline-secondary text-dark w-100 p-2" href="mailto:kuangsl@foxmail.com"
        target="_blank">
        <i class="iconfont icon-discussion"></i><br />纠错与咨询
    </a>
</div>
						<nav id="TableOfContents">
  <ul>
    <li><a href="#51-monte-carlo-prediction">5.1 Monte Carlo Prediction</a></li>
    <li><a href="#52-monte-carlo-estimation-of-action-values">5.2 Monte Carlo Estimation of Action Values</a></li>
    <li><a href="#53-monte-carlo-control">5.3 Monte Carlo Control</a></li>
    <li><a href="#54-monte-carlo-control-without-exploring-starts">5.4 Monte Carlo Control without Exploring Starts</a></li>
    <li><a href="#55-off-policy-prediction-via-importance-sampling">5.5 Off-policy Prediction via Importance Sampling</a></li>
    <li><a href="#56-incremental-implementation">5.6 Incremental Implementation</a></li>
    <li><a href="#57-off-policy-monte-carlo-control">5.7 Off-policy Monte Carlo Control</a></li>
  </ul>
</nav>
					</div>
				</div>
			</div>
		</div>
	</div>

	<script type="text/javascript" src="/js/jquery-3.4.1.min.js"></script>
<script type="text/javascript" src="/js/bootstrap.bundle.min.js"></script>
<script type="text/javascript" src="/js/leon.js"></script>
<script type="text/javascript" src="/js/TweenMax.min.js "></script>
<script type="text/javascript" src="/js/search.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"
    integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    
<script type="text/javascript">
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(
            document.body, {
                delimiters: [{
                        left: "$$",
                        right: "$$",
                        display: true
                    },
                    {
                        left: "\\[",
                        right: "\\]",
                        display: true
                    },
                    {
                        left: "$",
                        right: "$",
                        display: false
                    },
                    {
                        left: "\\(",
                        right: "\\)",
                        display: false
                    }
                ]
            }
        );
    });
</script>

<script type="text/javascript">
    $(document).on('click', 'a[href^="#"]', function (event) {
        event.preventDefault();

        $('html, body').animate({
            scrollTop: $($.attr(this, 'href')).offset().top
        }, 500);
    });
</script>


<script type="text/javascript">
    $(function () {
        $("[data-toggle='tooltip']").tooltip();
    });
</script>


</body>

</html>