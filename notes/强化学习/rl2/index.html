<!DOCTYPE html>
<html lang="zh-CN">

<head>
	<meta name="generator" content="Hugo 0.62.0" />
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="author" content="TristonK ">
<meta name="description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 2 这个读书笔记建立在已经看完了西瓜书第16章的基础上，略过了很多" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />

<link rel="canonical" href="http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/" />

<meta itemprop="name" content="Multi-armed Bandits">
<meta itemprop="description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 2 这个读书笔记建立在已经看完了西瓜书第16章的基础上，略过了很多">

<meta itemprop="wordCount" content="1890">



<meta itemprop="keywords" content="" />
<meta property="og:title" content="Multi-armed Bandits" />
<meta property="og:description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 2 这个读书笔记建立在已经看完了西瓜书第16章的基础上，略过了很多" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Multi-armed Bandits"/>
<meta name="twitter:description" content="对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 2 这个读书笔记建立在已经看完了西瓜书第16章的基础上，略过了很多"/>


<link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
<link rel="manifest" href="/icons/site.webmanifest">
<link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-config" content="/icons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

<title>Multi-armed Bandits</title>


<link rel="stylesheet" href="//at.alicdn.com/t/font_1559566_st54q8xrgt9.css">


    <link rel="stylesheet" href="/katex.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq">



    
    <link rel="stylesheet" href="/sass/main.min.8b65c41481eca5e77b1b5b1d62fdac20073e0b9bfdcba7497c50e773f61cc2cd.min.ffa84b8aebfb7bfa7bf67cadd8d153d6a4d0c616a8613d4743dca98e9db9447c.css" integity="sha256-/6hLiuv7e/p79nyt2NFT1qTQxhaoYT1HQ9ypjp25RHw=">

</head>

<body style="overflow-x: unset;">
	<div class="container-fluid">
		<div class="row d-print-block">
			<div class="col-12 col-md-3 col-lg-2 bd-sidebar d-print-none">
				<div class="d-flex mt-3 border-bottom">
        <span class="navbar-brand w-100" style="display: grid;">
            <small>
                <a href="/" class="text-black-50">
                    <i class="iconfont icon-back-arrow-"></i>
                </a>
                TristonK's
            </small>
            <a class="text-dark" href="/notes/">
                Notes
            </a>
        </span>
        <button class="btn btn-link text-dark d-md-none p-0 ml-3" type="button" data-toggle="collapse"
            data-target="#bd-docs-nav" aria-controls="bd-docs-nav" aria-expanded="true"
            aria-label="Toggle docs navigation">
            <i class="fad fa-bars"></i>
        </button>
    </div>
				<nav id="bd-docs-nav" class="collapse bd-links">
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/google%E4%BB%A3%E7%A0%81%E8%A7%84%E8%8C%83/">
            
            
            
            Google代码规范
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A33/">
            
            
            
            问题求解（三）
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A34/">
            
            
            
            问题求解（四）
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">
            
            
            
            操作系统
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%A7%81%E4%BA%BA%E7%94%9F%E6%B4%BB%E7%9A%84%E5%8F%98%E9%9D%A9/">
            
            
            
            私人生活的变革
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E5%BE%AE%E7%94%B5%E5%AD%90%E4%B8%8E%E7%94%B5%E8%B7%AF/">
            
            
            
            微电子与电路
        </a>
    </div>
    
    
    
    <div class="bd-toc-item active bg-light">
        <a class="bd-toc-link" href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
            <i class="iconfont icon-back-arrow-reverse"></i>
            强化学习
        </a>
        <ul class="nav bd-sidenav">
            
            
            <li class="active">
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/">
                    <i class="fad fa-chevron-right mr-1"></i>
                    Multi-armed Bandits
                </a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl3/">Finite Markov Decision Processes</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl4/">Dynamic Programming</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl5/">Monte Carlo Methods</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl6/">Temporal-Difference Learning</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl7/">n-step Bootstraping</a>
            </li>
            
            
            
            <li>
                <a href="/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/">强化学习-介绍</a>
            </li>
            
            
        </ul>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%A4%BE%E4%BC%9A%E5%AD%A6%E6%A6%82%E8%AE%BA/">
            
            
            
            社会学概论
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/">
            
            
            
            组合数学
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A6%82%E8%AE%BA/">
            
            
            
            数据库概论
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">
            
            
            
            计算机网络
        </a>
    </div>
    
    
    
    <div class="bd-toc-item">
        <a class="bd-toc-link" href="/notes/%E4%BC%9F%E5%A4%A7%E4%BC%A0%E7%BB%9F%E7%B3%BB%E5%88%97-%E5%8D%8E%E5%A4%8F%E5%87%BA%E7%89%88%E7%A4%BE/">
            
            
            
            伟大传统系列
        </a>
    </div>
    
    
</nav>
			</div>
			<div class="col-12 col-md-9 col-lg-10 d-print-block">
				<div class="row d-print-block">
					<main class="col-12 col-md-10 col-lg-9 py-md-3 pl-md-5 bd-content d-print-block" role="main">
						<div id="title" class="my-4 border-bottom">
							<span>强化学习</span>
							<h2>Multi-armed Bandits</h2>
							<footer>
								<span>
									<i class="iconfont icon-NewFile mr-2"></i>
									0001-01-01 08:00 CST
								</span> <br />
								<span>
									<i class="iconfont icon-modify mr-2"></i>
									0001-01-01 08:00 CST
								</span> <br />
								<span>
									<i class="iconfont icon-copyright mr-2"></i>
									CC BY-NC 4.0
								</span>
							</footer>
						</div>
						<div id="content" class="hl-h2">
							
							
							
							
							
							
							
							
							<blockquote>
<p>对应章节：<a href="http://www.incompleteideas.net/book/the-book-2nd.html">《Reinforcement Learning: An Introduction》第二版</a>  Chap 2</p>
</blockquote>
<p><strong>这个读书笔记建立在已经看完了西瓜书第16章的基础上，略过了很多东西</strong></p>
<blockquote>
<p>部分该章节西瓜书笔记可<a href="https://shelah-kuang.github.io/docs/RLzzh.pdf">点击</a>, <strong>因为太懒了，后面一部分的笔记还没<del>抄</del></strong></p>
</blockquote>
<p><strong><code>nonsassociative</code></strong> problem: one that does not involve learning to act in more than one situation
<strong><code>associative</code></strong> problem:  when actions are taken in more than one situation</p>
<p>本章主要围绕Multi-armed Bandits（多臂老虎机问题）展开讨论，以此为例介绍几种算法</p>
<h2 id="21-a-k-armed-bandit-problem">2.1 A k-armed Bandit Problem</h2>
<h3 id="problem-definition">Problem definition</h3>
<p>You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period</p>
<h3 id="heading">记号</h3>
<p>第t步的动作：$A_t$, 对应奖赏$R_t$
对于一个动作，预期奖赏为$q_*(a) = E[R_t | A_t = a]$
第t步选择动作a的估计奖赏为$Q_t(a)$</p>
<p>选择最优：exploiting 选择非最优：exploring</p>
<p>_然后就是探索与利用的关系，看西瓜书就行_</p>
<h2 id="22-action-value-methods">2.2 Action-value Methods</h2>
<p>计算Q：取平均值</p>
<p>有多种方式求解：</p>
<ol>
<li>greedy（全利用）</li>
<li>$\epsilon - greedy$</li>
</ol>
<h2 id="23-the-10-armed-testbed">2.3 The 10-armed Testbed</h2>
<p>比较了一下$\epsilon - greedy$与$greedy$的差异，优缺点以及优化看西瓜书即可</p>
<h2 id="24-incremental-implementation">2.4 Incremental Implementation</h2>
<p>如何快速更新Q估计值：
$Q_{n+1} = Q_n + 1/n(R_n-Q_n)$</p>
<p>上为以下公式的一种形式</p>
<p>$$NewEstimate\leftarrow OldEstimate + StepSize [Target − OldEstimate]$$</p>
<p>$[Target − OldEstimate] $被称为是估计中的$error$
将$StepSize$表示为$\alpha$, 更一般的，表示为$\alpha_t(a)$</p>
<h2 id="25-tracking-a-nonstationary-problem">2.5 Tracking a Nonstationary Problem</h2>
<p><strong><code>stationary</code></strong>: the reward probabilities do not change over time</p>
<p>对于Nonstationary的问题，我们要给更多的权重给最近得到的rewards。
一种常见的方式是给$\alpha$设置一个常数值（展开可知$Q_{n+1} = (1-\alpha)^nQ_1+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i$），我们称其为<code>weighted average</code>，因为其权重和为1.有时候也称其为<code>exponential recency-weighted average</code>.</p>
<p>有时候让$\alpha$随时间变化而变化，用$\alpha_n(a)$第n次选择动作a时的stepsize。
要保证在数量足够大的情况下能够让估计值趋向于真实值，根据随机近似理论(stochastic approximation),要满足以下条件：</p>
<p>$$\sum_{n=1}^\infin a_n(a) = \infin ,,\sum_{n=1}^\infin a_n^2(a) &lt; \infin $$</p>
<p>The first condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random fluctuations. The second condition guarantees that eventually the steps become small enough to assure convergence.</p>
<p>可以看出我们之前让$\alpha$为一个常数时是满足第一个条件而不满足第二个条件的，这导致其会一直变化，这也正是我们希望在Nonstationary的情况下做到的</p>
<p>值得说明的是，满足两个条件的参数一般都收敛的比较慢，需要仔细的调参。故而虽然在理论分析时考虑这个，但是实际应用时很少考虑这个</p>
<h2 id="26-optimistic-initial-values">2.6 Optimistic Initial Values</h2>
<p>之前提到的所有的方式都某种程度上依赖于初始值的估计$Q_1(a)$, 可以说这些方式都有初始值的偏差（biased by their initial estimates.）对于sample-average methods而言，只需要每个动作试一遍就可以消除这种误差了，但是对于step size为常数的方式而言，这种误差是一直存在的（虽然会不断缩小，理由可见表达式）。</p>
<p>初始值有时也可以被用于促进exploration，如将其初始值设定的比估计值都大，那么就可以使得会优先把所有的动作都尝试一遍，我们称呼这种为<strong><code>Optimistic Initial Values</code></strong></p>
<p>但这种方式不适用于nonstationary的情况，因为任务一旦变化，所有的初始值都不可能始终适用</p>
<h2 id="27-upper-confidence-bound-action-selection">2.7 Upper-Confidence-Bound Action Selection</h2>
<p>探索是必要的， 但是$\epsilon - greedy$并没有给出一个偏向性，使得尽可能去探索接近greedy的或者是探索次数还不够多导致并不确定的动作。所以有了<strong><code>upper confidence bound (UCB) action selection</code></strong></p>
<p>$$A_t = arg,max[Q_t(a)+c\sqrt{\frac{\ln t }{N_t(a)}}]$$</p>
<p>其中$N_t(a)$表示动作a在第t次前被执行的次数，the number c &gt; 0 controls the degree of exploration，c可以看作是对于估计的信任程度</p>
<blockquote>
<p>UCB often performs well, as shown here, but is more difficult than &ldquo;-greedy to extend beyond bandits
to the more general reinforcement learning settings considered in the rest of this book.
One difficulty is in dealing with nonstationary problems; methods more complex than those presented in Section 2.5 would be needed. Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book. In these more advanced settings the idea of UCB action selection is usually not practical.</p>
</blockquote>
<h2 id="28-gradient-bandit-algorithms">2.8 Gradient Bandit Algorithms</h2>
<p>构建一个数值类型的函数$H_t(a)$使得对于每一个动作a有一定的偏向性</p>
<p><strong><code>soft-max distribution</code></strong>:</p>
<p>$$Pr{A_t = a} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)$$</p>
<p>对于第t步选择了$A_t$之后，更新各个概率为</p>
<p>$H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \overline{R_t})(1-\pi_t(A_t))$  and</p>
<p>$H_{t+1}(a) = H_t(a) - \alpha (R_t - \overline{R_t})\pi_t(a)$</p>
<p>其中$ \overline{R_t}$ 表示除去第t步之外的所有奖赏的平均值，其相当于一个baseline，如果奖赏高于这个baseline，那么对应动作$A_t$的在未来被选择的概率将增加，否则将减少</p>
<h2 id="29-associative-search-contextual-bandits">2.9 Associative Search (Contextual Bandits)</h2>
<blockquote>
<p>However, in a general reinforcement learning task there is more than one situation, and the goal
is to learn a policy: a mapping from situations to the actions that are best in those situations.</p>
</blockquote>
<blockquote>
<p>associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature.</p>
</blockquote>
<blockquote>
<p>Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem.If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem.</p>
</blockquote>

						</div>
					</main>
					<div class="d-none d-lg-block col-lg-3 bd-toc d-print-none">
						<div class="btn-group-vertical w-100 my-3">
    

    
    
    
    
        <a class="btn btn-outline-secondary text-dark w-100 p-2" href="#" onclick="window.print()">
            <i class="iconfont icon-dayin"></i><br />打印本页
        </a>
    
    
</div>
						<h4 class="card-title pb-0">目录</h4>
						<nav id="TableOfContents">
  <ul>
    <li><a href="#21-a-k-armed-bandit-problem">2.1 A k-armed Bandit Problem</a>
      <ul>
        <li><a href="#problem-definition">Problem definition</a></li>
        <li><a href="#heading">记号</a></li>
      </ul>
    </li>
    <li><a href="#22-action-value-methods">2.2 Action-value Methods</a></li>
    <li><a href="#23-the-10-armed-testbed">2.3 The 10-armed Testbed</a></li>
    <li><a href="#24-incremental-implementation">2.4 Incremental Implementation</a></li>
    <li><a href="#25-tracking-a-nonstationary-problem">2.5 Tracking a Nonstationary Problem</a></li>
    <li><a href="#26-optimistic-initial-values">2.6 Optimistic Initial Values</a></li>
    <li><a href="#27-upper-confidence-bound-action-selection">2.7 Upper-Confidence-Bound Action Selection</a></li>
    <li><a href="#28-gradient-bandit-algorithms">2.8 Gradient Bandit Algorithms</a></li>
    <li><a href="#29-associative-search-contextual-bandits">2.9 Associative Search (Contextual Bandits)</a></li>
  </ul>
</nav>
						
						
					</div>
				</div>
			</div>
		</div>
	</div>

	<script type="text/javascript" src="/jquery.min.07215acd0f8985210d3d19ffd365d89d1a38612a5609bbb0d736efa9073e9d25173b2d133e16bb2f4692d082841262e5464ec62a38169dfda7e631e62a8799e3.js" integrity="sha512-ByFazQ&#43;JhSENPRn/02XYnRo4YSpWCbuw1zbvqQc&#43;nSUXOy0TPha7L0aS0IKEEmLlRk7GKjgWnf2n5jHmKoeZ4w=="></script><script type="text/javascript" src="/bootstrap.min.94cdf6adacf71def2df3c6ad389ad4ffab5384da3c495482462a211f4c91d5dae543a7b97baac3b345b964af4dee64d85f550ed232decc8dda1162fccc7e9f93.js" integrity="sha512-lM32raz3He8t88atOJrU/6tThNo8SVSCRiohH0yR1drlQ6e5e6rDs0W5ZK9N7mTYX1UO0jLezI3aEWL8zH6fkw=="></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
onload="renderMathInElement(document.body);"></script>

<script type="text/javascript" src="/custom.min.d3e1b7647f32dbe7e0140398739a26dad3f3470fc1eebe0741ef33668f1b7bd0b2917dc6efb9f0d9f1092b91dca502cab1b883863f02530133a8a8ef609926af.js" integrity="sha512-0&#43;G3ZH8y2&#43;fgFAOYc5om2tPzRw/B7r4HQe8zZo8be9CykX3G77nw2fEJK5HcpQLKsbiDhj8CUwEzqKjvYJkmrw=="></script>
<script type="text/javascript">

document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(
        document.body, {
            delimiters: [
                {
                    left: "$$",
                    right: "$$",
                    display: true
                },
                {
                    left: "\\[",
                    right: "\\]",
                    display: true
                },
                {
                    left: "$",
                    right: "$",
                    display: false
                },
                {
                    left: "\\(",
                    right: "\\)",
                    display: false
                }
            ],
            strict: false
        }
    );
});


$(document).on('click', 'a[href^="#"]', function (event) {
    event.preventDefault();

    $('html, body').animate({
        scrollTop: $($.attr(this, 'href')).offset().top
    }, 500);
});
</script>




</body>

</html>