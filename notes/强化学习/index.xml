<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习 on Tristonk</title>
    <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习 on Tristonk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <copyright>CC BY-NC 4.0</copyright>
    <lastBuildDate>Fri, 03 Jan 2020 20:18:52 +0800</lastBuildDate>
    
	<atom:link href="http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Multi-armed Bandits</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl2/</guid>
      <description>对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 2 这个读书笔记建立在已经看完了西瓜书第16章的基础上，略过了很多</description>
    </item>
    
    <item>
      <title>Finite Markov Decision Processes</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl3/</guid>
      <description>对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 3 感觉这一章就给了一些基础概念，看西瓜书也成啊 3.1 The Agent–E</description>
    </item>
    
    <item>
      <title>Dynamic Programming</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl4/</guid>
      <description>对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 4 The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. 如果看完了西</description>
    </item>
    
    <item>
      <title>Monte Carlo Methods</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl5/</guid>
      <description>对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 5 蒙特卡洛方法不需要环境的信息，只需要经验（experience</description>
    </item>
    
    <item>
      <title>Temporal-Difference Learning</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl6/</guid>
      <description>对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 6 TD：时序差分方法，是一种综合了蒙特卡洛与DP的特点的方法 6.1 TD</description>
    </item>
    
    <item>
      <title>n-step Bootstraping</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/rl7/</guid>
      <description>对应章节：《Reinforcement Learning: An Introduction》第二版 Chap 7</description>
    </item>
    
    <item>
      <title>强化学习-介绍</title>
      <link>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://shelah-kuang.github.io/notes/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/</guid>
      <description>对应教材：Sutton &amp;amp; Barto&#39;s book Reinforcement Learning: An Introduction (2nd Edition) Chap1 Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal The history is the sequence of observations, actions, rewards $$Ht = O_1, R_1, A_1, &amp;hellip;, A</description>
    </item>
    
  </channel>
</rss>